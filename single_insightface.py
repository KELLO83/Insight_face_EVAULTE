import os
import itertools
import random
import numpy as np
import pandas as pd
from insightface.app import FaceAnalysis
import cv2
from tqdm import tqdm
from sklearn.metrics import roc_curve, auc, confusion_matrix
import logging
import traceback
import pickle
import argparse
import matplotlib.pyplot as plt

# --- ë¡œê¹… ë° ê²½ë¡œ ì„¤ì • ---
try:
    script_dir = os.path.dirname(os.path.abspath(__file__))
except NameError:
    script_dir = os.getcwd()

def get_log_file(model_name):
    """ëª¨ë¸ëª…ì— ë”°ë¥¸ ë¡œê·¸ íŒŒì¼ ê²½ë¡œ ë°˜í™˜"""
    return os.path.join(script_dir, f"{model_name}_LOG.txt")

model_info = {
        "antelopev2": "ResNet-100 + Glint360K (407MB)",
        "buffalo_l": "ResNet-50 + WebFace600K (326MB)", 
        "buffalo_m": "ResNet-50 + WebFace600K (313MB)",
        "buffalo_s": "MobileFaceNet + WebFace600K (159MB)",
        "buffalo_sc": "MobileFaceNet + WebFace600K (16MB)",
    }

def get_all_embeddings_insightface(identity_map, face_app, model_name, dataset_name, use_cache=True):
    """InsightFaceë¥¼ ì‚¬ìš©í•œ ì„ë² ë”© ì¶”ì¶œ (normed_embedding ì‚¬ìš©)"""
    cache_file = os.path.join(script_dir, f"embeddings_cache_{dataset_name}_{model_name}_insightface.pkl")
    print(f"\nìºì‹œ íŒŒì¼: {cache_file}")
    
    def normalize_path(path):
        if 'pair/' in path:
            return path.split('pair/')[-1]
        return path
    
    if use_cache and os.path.exists(cache_file):
        print(f"\nìºì‹œ íŒŒì¼ '{cache_file}'ì—ì„œ ì„ë² ë”©ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
        with open(cache_file, 'rb') as f:
            cached_embeddings = pickle.load(f)
        
 
        embeddings = {}
        path_mapping = {}  
        for cached_path, embedding in cached_embeddings.items():
            normalized_cached = normalize_path(cached_path)
            path_mapping[normalized_cached] = cached_path
            
        all_current_images = sorted(list(set(itertools.chain.from_iterable(identity_map.values()))))
        
        for current_path in all_current_images:
            normalized_current = normalize_path(current_path)
            if normalized_current in path_mapping:
                original_cached_path = path_mapping[normalized_current]
                embeddings[current_path] = cached_embeddings[original_cached_path]
            else:
                embeddings[current_path] = None
                
        print(f"ê²½ë¡œ ë§¤ì¹­ ì™„ë£Œ: {len([v for v in embeddings.values() if v is not None])}ê°œ ì„±ê³µ")
        print("ì„ë² ë”© ë¡œë“œ ì™„ë£Œ.")
        return embeddings

    all_images = sorted(list(set(itertools.chain.from_iterable(identity_map.values()))))
    print(f"\nì´ {len(all_images)}ê°œì˜ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„ë² ë”©ì„ ìƒˆë¡œ ì¶”ì¶œí•©ë‹ˆë‹¤...")
    
    embeddings = {}
    
    for img_path in tqdm(all_images, desc="ì„ë² ë”© ì¶”ì¶œ"):
        try:
            image = cv2.imread(img_path)
            if image is None:
                logging.warning(f"ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: {img_path}")
                embeddings[img_path] = None
                continue
            
            image_rgb = image[:, :, ::-1]
            faces = face_app.get(image_rgb)
            
            if len(faces) == 0:
                logging.warning(f"ì–¼êµ´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {img_path}")
                embeddings[img_path] = None
            else:
                embeddings[img_path] = faces[0].normed_embedding
                
        except Exception as e:
            logging.warning(f"ì„ë² ë”© ì¶”ì¶œ ì˜¤ë¥˜: {img_path}. ì œì™¸ë©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
            embeddings[img_path] = None

    if use_cache:
        print(f"\nì¶”ì¶œëœ ì„ë² ë”©ì„ ìºì‹œ íŒŒì¼ '{cache_file}'ì— ì €ì¥í•©ë‹ˆë‹¤...")
        with open(cache_file, 'wb') as f:
            pickle.dump(embeddings, f)
        print("ìºì‹œ ì €ì¥ ì™„ë£Œ.")
    return embeddings

def collect_scores_from_embeddings(pairs, embeddings, is_positive):
    """ì„ë² ë”©ìœ¼ë¡œ ìœ ì‚¬ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤ (np.dot ì‚¬ìš©)."""
    similarities, labels = [], []
    label = 1 if is_positive else 0
    desc = "ë™ì¼ ì¸ë¬¼ ìŒ ê³„ì‚°" if is_positive else "ë‹¤ë¥¸ ì¸ë¬¼ ìŒ ê³„ì‚°"
    for img1_path, img2_path in tqdm(pairs, desc=desc):
        emb1, emb2 = embeddings.get(img1_path), embeddings.get(img2_path)
        if emb1 is not None and emb2 is not None:
            similarities.append(np.dot(emb1, emb2))
            labels.append(label)
            
    return similarities, labels

def save_results_to_excel(excel_path, model_name, roc_auc, eer, tar_at_far_results, target_fars, metrics, total_dataset_img_len, total_class,
                           data_path, model_attr_value):
    """ê²°ê³¼ë¥¼ Excel íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    new_data = {
        "model_name": [model_name],
        "roc_auc": [f"{roc_auc:.4f}"], "eer": [f"{eer:.4f}"],
        "accuracy": [f"{metrics['accuracy']:.4f}"], "recall": [f"{metrics['recall']:.4f}"],
        "f1_score": [f"{metrics['f1_score']:.4f}"], "tp": [metrics['tp']],
        "tn": [metrics['tn']], "fp": [metrics['fp']], "fn": [metrics['fn']]
    }

    for far in target_fars:
        new_data[f"tar_at_far_{far*100:g}%"] = [f"{tar_at_far_results.get(far, 0):.4f}"]

    new_data.update({
        'total_dataset_img_len': [total_dataset_img_len],
        'total_class': [total_class],
        'data_path': [data_path],
        'model_attr': [model_attr_value]
    })
    
    new_df = pd.DataFrame(new_data)
    try:
        df = pd.read_excel(excel_path)
    except FileNotFoundError:
        df = pd.DataFrame()

    updated_df = pd.concat([df, new_df], ignore_index=True)
    updated_df.to_excel(excel_path, index=False)
    print(f"\ní‰ê°€ ê²°ê³¼ê°€ '{excel_path}' íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def plot_roc_curve(fpr, tpr, roc_auc, model_name, excel_path):
    """ROC ì»¤ë¸Œë¥¼ ê·¸ë¦¬ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""

    modeL_value = model_info.get(model_name, "ì •ë³´ ì—†ìŒ")
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.4f})')
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate (FAR)')
    plt.ylabel('True Positive Rate (TAR)')
    plt.title(f'ROC Curve {model_name} {modeL_value}')
    plt.legend(loc="lower right")
    plt.grid(True)
    plot_filename = os.path.splitext(excel_path)[0] + f"_{model_name}_roc_curve.png"
    plt.savefig(plot_filename)
    print(f"ROC ì»¤ë¸Œ ê·¸ë˜í”„ê°€ '{plot_filename}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

def main(args):
    # ëª¨ë¸ëª…ì— ë”°ë¥¸ ë¡œê·¸ íŒŒì¼ ì„¤ì •
    LOG_FILE = get_log_file(args.model_name)
    logging.basicConfig(
        filename=LOG_FILE, level=logging.WARNING,
        format='%(asctime)s - %(levelname)s - %(message)s', filemode='w'
    )
    
    if not os.path.isdir(args.data_path):
        raise FileNotFoundError(f"ë°ì´í„°ì…‹ ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {args.data_path}")

    identity_map = {}
    for person_folder in os.listdir(args.data_path):
        person_path = os.path.join(args.data_path, person_folder)
        if os.path.isdir(person_path):
            images = [os.path.join(person_path, f) for f in os.listdir(person_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
            if len(images) > 1:
                identity_map[person_folder] = images
    
    if not identity_map:
        raise ValueError("ë°ì´í„°ì…‹ì—ì„œ 2ê°œ ì´ìƒì˜ ì´ë¯¸ì§€ë¥¼ ê°€ì§„ ì¸ë¬¼ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    print(f"ì´ {len(identity_map)}ëª…ì˜ ì¸ë¬¼, {sum(len(v) for v in identity_map.values())}ê°œì˜ ì´ë¯¸ì§€ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤.")

    print("\ní‰ê°€ì— ì‚¬ìš©í•  ë™ì¼ ì¸ë¬¼/ë‹¤ë¥¸ ì¸ë¬¼ ìŒì„ ìƒì„±í•©ë‹ˆë‹¤...")
    
    positive_pairs = []
    for imgs in tqdm(identity_map.values(), desc="ë™ì¼ ì¸ë¬¼ ìŒ ìƒì„±"):
        positive_pairs.extend(itertools.combinations(imgs, 2))

    num_positive_pairs = len(positive_pairs)
    
    identities = list(identity_map.keys())
    negative_pairs_set = set()
    if len(identities) > 1:
        with tqdm(total=num_positive_pairs, desc="ë‹¤ë¥¸ ì¸ë¬¼ ìŒ ìƒì„±") as pbar:
            while len(negative_pairs_set) < num_positive_pairs:
                id1, id2 = random.sample(identities, 2)
                pair = (random.choice(identity_map[id1]), random.choice(identity_map[id2]))
                sorted_pair = tuple(sorted(pair))
                if sorted_pair not in negative_pairs_set:
                    negative_pairs_set.add(sorted_pair)
                    pbar.update(1)
    negative_pairs = list(negative_pairs_set)

    print(f"- ë™ì¼ ì¸ë¬¼ ìŒ: {len(positive_pairs)}ê°œ, ë‹¤ë¥¸ ì¸ë¬¼ ìŒ: {len(negative_pairs)}ê°œ")
    print(f"\{args.model_name} ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")

    provider_options = [
        {},  # CUDAExecutionProviderì— ëŒ€í•œ ì˜µì…˜
        {'intra_op_num_threads': 0}  # CPUExecutionProviderì— ëŒ€í•œ ì˜µì…˜
    ]
        
    face_app = FaceAnalysis(
        name=args.model_name,
        providers=["CUDAExecutionProvider", "CPUExecutionProvider"],
        root=".",
        provider_options=provider_options
    )

    face_app.prepare(ctx_id=0, det_size=tuple(args.det_size))
    print("ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")

    dataset_name = os.path.basename(os.path.normpath(args.data_path))
    embeddings = get_all_embeddings_insightface(
        identity_map, face_app, args.model_name, dataset_name, 
        use_cache=args.cache
    )

    pos_similarities, pos_labels = collect_scores_from_embeddings(positive_pairs, embeddings, is_positive=True)
    neg_similarities, neg_labels = collect_scores_from_embeddings(negative_pairs, embeddings, is_positive=False)

    print(f"   - ì „ì²´ ì„ë² ë”© ìˆ˜: {len(embeddings)}")
    print(f"   - ìœ íš¨í•œ ì„ë² ë”© ìˆ˜: {sum(1 for v in embeddings.values() if v is not None)}")
    print(f"   - None ì„ë² ë”© ìˆ˜: {sum(1 for v in embeddings.values() if v is None)}")
    print(f"   - ì–‘ì„± ìŒ ìœ ì‚¬ë„ ìˆ˜: {len(pos_similarities)}")
    print(f"   - ìŒì„± ìŒ ìœ ì‚¬ë„ ìˆ˜: {len(neg_similarities)}")
    print(f"\n--- ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„ ---")

    if pos_similarities and neg_similarities:
        print(f"ğŸ”µ ë™ì¼ ì¸ë¬¼ ìŒ ìœ ì‚¬ë„:")
        print(f"   - ìµœì†Œê°’: {min(pos_similarities):.4f}")
        print(f"   - ìµœëŒ€ê°’: {max(pos_similarities):.4f}")
        print(f"   - í‰ê· ê°’: {np.mean(pos_similarities):.4f}")
        print(f"   - í‘œì¤€í¸ì°¨: {np.std(pos_similarities):.4f}")
        
        print(f"ğŸ”´ ë‹¤ë¥¸ ì¸ë¬¼ ìŒ ìœ ì‚¬ë„:")
        print(f"   - ìµœì†Œê°’: {min(neg_similarities):.4f}")
        print(f"   - ìµœëŒ€ê°’: {max(neg_similarities):.4f}")
        print(f"   - í‰ê· ê°’: {np.mean(neg_similarities):.4f}")
        print(f"   - í‘œì¤€í¸ì°¨: {np.std(neg_similarities):.4f}")
    
    scores = np.array(pos_similarities + neg_similarities)
    labels = np.array(pos_labels + neg_labels)

    print("\n--- ìµœì¢… í‰ê°€ ê²°ê³¼ ---")
    if labels.size > 0:
        fpr, tpr, thresholds = roc_curve(labels, scores)
        roc_auc = auc(fpr, tpr)
        
        frr = 1 - tpr
        eer_index = np.nanargmin(np.abs(fpr - frr))
        eer = fpr[eer_index]
        eer_threshold = thresholds[eer_index]

        tar_at_far_results = {far: np.interp(far, fpr, tpr) for far in args.target_fars}
        
        predictions = (scores >= eer_threshold).astype(int)
        cm = confusion_matrix(labels, predictions)
        
        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0,0,0,0)

        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        metrics = {"accuracy": accuracy, "recall": recall, "f1_score": f1_score, "tp": tp, "tn": tn, "fp": fp, "fn": fn}

        print(f"ì‚¬ìš©ëœ ëª¨ë¸: InsightFace {args.model_name}, ì „ì²´ í‰ê°€ ìŒ: {len(labels)} ê°œ")
        print(f"[ì£¼ìš” ì„±ëŠ¥] ROC-AUC: {roc_auc:.4f}, EER: {eer:.4f} (ìœ ì‚¬ë„ ì„ê³„ê°’: {eer_threshold:.4f})")
        print(f"[ìƒì„¸ ì§€í‘œ] Accuracy: {metrics['accuracy']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1_score']:.4f}")
        for far, tar in tar_at_far_results.items():
            print(f"  - TAR @ FAR {far*100:g}%: {tar:.4f}")
        
        with open(LOG_FILE, 'a') as log_file:
            log_file.write(f"\nInsightFace {args.model_name} í‰ê°€ ê²°ê³¼:\n")
            log_file.write(f"ROC-AUC: {roc_auc:.4f}, EER: {eer:.4f} (Threshold: {eer_threshold:.4f})\n")
            log_file.write(f"Accuracy: {metrics['accuracy']:.4f}, Recall: {metrics['recall']:.4f}, F1-Score: {metrics['f1_score']:.4f}\n")
            for far, tar in tar_at_far_results.items():
                log_file.write(f"TAR @ FAR {far*100:g}%: {tar:.4f}\n")

        excel_path = os.path.join(script_dir, args.excel_path)
        total_dataset_img_len = sum(len(v) for v in identity_map.values())
        total_class = len(identity_map)
        model_attr_value = model_info.get(args.model_name, "ì •ë³´ ì—†ìŒ")
        save_results_to_excel(excel_path, f"InsightFace_{args.model_name}", roc_auc, eer, tar_at_far_results, \
                              args.target_fars, metrics, total_dataset_img_len, total_class, args.data_path, model_attr_value)
        
        
        if args.plot_roc:
            plot_roc_curve(fpr, tpr, roc_auc, args.model_name, excel_path)
    else:
        msg = "í‰ê°€ë¥¼ ìœ„í•œ ìœ íš¨í•œ ì ìˆ˜ë¥¼ ìˆ˜ì§‘í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        print(msg)
        logging.error(msg)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Single-Process InsightFace Evaluation Script")
    parser.add_argument("--data_path", type=str, default="lfw_sorting", help="í‰ê°€í•  ë°ì´í„°ì…‹ì˜ ë£¨íŠ¸ í´ë”")
    parser.add_argument("--model_name", type=str, default="buffalo_l", 
                       choices=["antelopev2", "buffalo_l", "buffalo_m", "buffalo_s", "buffalo_sc","auraface"],
                       help="ì‚¬ìš©í•  InsightFace ëª¨ë¸")
    parser.add_argument("--excel_path", type=str, default="insightface_evaluation_results.xlsx", help="ê²°ê³¼ë¥¼ ì €ì¥í•  Excel íŒŒì¼ ì´ë¦„")
    parser.add_argument("--target_fars", nargs='+', type=float, default=[0.01, 0.001, 0.0001], help="TARì„ ê³„ì‚°í•  FAR ëª©í‘œê°’ë“¤")
    parser.add_argument("--det_size", nargs=2, type=int, default=[640, 640], help="ì–¼êµ´ ê²€ì¶œ í¬ê¸° (width, height)")
    parser.add_argument("--cache", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ê¸°ì¡´ ì„ë² ë”© ìºì‹œ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ ë¹ ë¥´ê²Œ ì‹¤í–‰í•©ë‹ˆë‹¤.", default=False) 
    parser.add_argument("--plot-roc", action="store_true", help="ì´ í”Œë˜ê·¸ë¥¼ ì‚¬ìš©í•˜ë©´ ROC ì»¤ë¸Œ ê·¸ë˜í”„ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.", default=True)
    args = parser.parse_args()

    try:
        main(args)
    except Exception as e:
        LOG_FILE = get_log_file(args.model_name)
        error_message = traceback.format_exc()
        logging.error(f"ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ ì¤‘ ì²˜ë¦¬ë˜ì§€ ì•Šì€ ì˜ˆì™¸ ë°œìƒ:\n{error_message}")
        print(f"\nì¹˜ëª…ì ì¸ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. '{LOG_FILE}' íŒŒì¼ì— ìƒì„¸ ë‚´ì—­ì´ ê¸°ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")